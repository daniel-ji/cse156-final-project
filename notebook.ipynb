{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjYOW8Ps9tds",
        "outputId": "cefd46c7-af77-44a2-bf33-b6dcef20dd32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvBJu-I9B0sq",
        "outputId": "cd6c3fe2-17f3-4a3c-a5e4-33020670c088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 multiprocess-0.70.16 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT6dA7W6GYfI"
      },
      "outputs": [],
      "source": [
        "!rm -rf predictions.txt\n",
        "!rm -rf raw_predictions.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M-xck2m-C3a",
        "outputId": "add04c05-d41e-45b2-ddc1-3b21a9dd65a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 0 samples\n",
            "Processed 100 samples\n",
            "Processed 200 samples\n",
            "Processed 300 samples\n",
            "Processed 400 samples\n",
            "Processed 500 samples\n",
            "Processed 600 samples\n",
            "Processed 700 samples\n",
            "Processed 800 samples\n",
            "Processed 900 samples\n",
            "Processed 1000 samples\n",
            "Processed 1100 samples\n",
            "Processed 1200 samples\n",
            "Processed 1300 samples\n",
            "Processed 1400 samples\n",
            "Processed 1500 samples\n",
            "Processed 1600 samples\n",
            "Processed 1700 samples\n",
            "Processed 1800 samples\n",
            "Processed 1900 samples\n",
            "Processed 2000 samples\n",
            "Processed 2100 samples\n",
            "Processed 2200 samples\n",
            "Processed 2300 samples\n",
            "Processed 2400 samples\n",
            "Processed 2500 samples\n",
            "Processed 2600 samples\n",
            "Processed 2700 samples\n",
            "Processed 2800 samples\n",
            "Processed 2900 samples\n",
            "Processed 3000 samples\n",
            "Processed 3100 samples\n",
            "Processed 3200 samples\n",
            "Processed 3300 samples\n",
            "Processed 3400 samples\n",
            "Processed 3500 samples\n",
            "Processed 3600 samples\n",
            "Processed 3700 samples\n",
            "Processed 3800 samples\n",
            "Processed 3900 samples\n",
            "Processed 4000 samples\n",
            "Processed 4100 samples\n",
            "Processed 4200 samples\n",
            "Processed 4300 samples\n",
            "Processed 4400 samples\n",
            "Processed 4500 samples\n",
            "Processed 4600 samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1002 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping sample with too long query\n",
            "Processed 4700 samples\n",
            "Processed 4800 samples\n",
            "Processed 4900 samples\n",
            "Processed 5000 samples\n",
            "Processed 5100 samples\n",
            "Processed 5200 samples\n",
            "Processed 5300 samples\n",
            "Processed 5400 samples\n",
            "Processed 5500 samples\n",
            "Processed 5600 samples\n",
            "Processed 5700 samples\n",
            "Processed 5800 samples\n",
            "Processed 5900 samples\n",
            "Processed 6000 samples\n",
            "Processed 6100 samples\n",
            "Processed 6200 samples\n",
            "Processed 6300 samples\n",
            "Processed 6400 samples\n",
            "Processed 6500 samples\n",
            "Processed 6600 samples\n",
            "Processed 6700 samples\n",
            "Processed 6800 samples\n",
            "Processed 6900 samples\n",
            "Processed 7000 samples\n",
            "Processed 7100 samples\n",
            "Processed 7200 samples\n",
            "Processed 7300 samples\n",
            "Processed 7400 samples\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "model = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\").cuda()\n",
        "\n",
        "# Load dataset\n",
        "dataset = None\n",
        "with open(\"dev.json\", 'r') as file:\n",
        "    dataset = json.load(file)['data']\n",
        "\n",
        "file_data = []\n",
        "\n",
        "count = 0\n",
        "for sample in dataset:\n",
        "    if not sample['qas'][0][\"answers\"] or len(sample['qas'][0][\"answers\"][0]) == 0:\n",
        "        print(\"Skipping sample with no answers\")\n",
        "        continue\n",
        "\n",
        "    # Ensure entities are considered for predictions\n",
        "    entities_indexes = sample['passage']['entities']\n",
        "    entities = []\n",
        "    for index in entities_indexes:\n",
        "        entities.append(sample['passage']['text'][index['start']:(index['end']+1)])\n",
        "\n",
        "    # Mask the query\n",
        "    masked_query = sample['qas'][0]['query'].replace(\"@placeholder\", \"<mask>\")\n",
        "    # Combine the passage and query\n",
        "    query = sample['passage']['text'] + \"\\n\\n\" + masked_query\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(query, return_tensors=\"pt\").to('cuda')\n",
        "        # Should be only one sample in the dev.json file with more than 512 tokens\n",
        "        if (len(inputs['input_ids'][0]) > 512):\n",
        "            print(\"Skipping sample with too long query\")\n",
        "            continue\n",
        "\n",
        "        # Get the logits from the model of the masked token\n",
        "        outputs = model(**inputs)\n",
        "        mask_token_index = (inputs['input_ids'] == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "        logits = outputs.logits[0, mask_token_index][0]\n",
        "\n",
        "        # Get logits for each entity\n",
        "        entity_ids = [tokenizer.encode(entity, add_special_tokens=False) for entity in entities]\n",
        "\n",
        "        # Calculate likelihoods for each entity (average of the logits for each token in the entity)\n",
        "        likelihoods = []\n",
        "        for entity_id in entity_ids:\n",
        "            likelihood = 0\n",
        "            for token in entity_id:\n",
        "                likelihood += logits[token]\n",
        "            likelihoods.append(likelihood / len(entity_id))\n",
        "\n",
        "        # Get the entity with the highest likelihood\n",
        "        prediction = entities[likelihoods.index(max(likelihoods))]\n",
        "\n",
        "        file_data.append((query, prediction, sample['qas'][0][\"answers\"][0][\"text\"]))\n",
        "\n",
        "        # Write to file and log progress, every 100 samples\n",
        "        if count % 100 == 0:\n",
        "            print(f\"Processed {count} samples\")\n",
        "            # Write the results to a file\n",
        "            with open(\"predictions.txt\", \"a\") as f:\n",
        "                for query, prediction, answer in file_data:\n",
        "                    f.write(f\"Query: {query}\\n\")\n",
        "                    f.write(f\"Prediction: {prediction}\\n\")\n",
        "                    f.write(f\"Answer: {answer}\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "\n",
        "            # Write raw results to a file (for further metric calculation)\n",
        "            with open(\"raw_predictions.tsv\", \"a\") as f:\n",
        "                for _, prediction, answer in file_data:\n",
        "                    f.write(f\"{prediction}\\t{answer}\\n\")\n",
        "\n",
        "            file_data = []\n",
        "\n",
        "        count += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJemClj6vEG5",
        "outputId": "1d28c4f8-1c09-4423-f1a3-87988b2e395f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def load_data(filename):\n",
        "    predictions, actuals = [], []\n",
        "    with open(filename, mode='r', newline='', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            if len(row) == 2:  # Ensure there are exactly two columns\n",
        "                predictions.append(row[0])\n",
        "                actuals.append(row[1])\n",
        "            else:\n",
        "                raise ValueError(\"Each row must contain exactly two columns.\")\n",
        "\n",
        "    return predictions, actuals\n",
        "\n",
        "def calculate_metrics(predictions, actuals):\n",
        "    f1 = f1_score(actuals, predictions, average='weighted')\n",
        "\n",
        "    # Exact match calculation\n",
        "    exact_matches = sum(1 for i in range(len(predictions)) if predictions[i] == actuals[i])\n",
        "    exact_match_score = exact_matches / len(predictions)\n",
        "\n",
        "    return f1, exact_match_score\n",
        "\n",
        "def main():\n",
        "    predictions, actuals = load_data('raw_predictions.tsv')\n",
        "    f1, exact_match_score = calculate_metrics(predictions, actuals)\n",
        "\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Exact Match Score:\", exact_match_score)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z1TjSPxu9yi",
        "outputId": "f0dbcf29-a2d6-4a33-db66-8f01ec3bb156"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.406759466760207\n",
            "Exact Match Score: 0.4403458992028104\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}